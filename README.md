# Attention Is All You Need: PyTorch Transformer Implementation

[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue)]()
[![License: MIT](https://img.shields.io/badge/license-MIT-green)]()

## ğŸš€ Project Overview

This repository contains a from-scratch PyTorch implementation of the original Transformer architecture from Vaswani et al.â€™s paper ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762). Building it myself was the hardest thing Iâ€™ve ever doneâ€”I still donâ€™t fully grok every detailâ€”but it deepened my understanding of modern sequence-to-sequence models (GPT, BERT, etc.). Along the way I learned how embeddings work, how the encoder uses Queries, Keys, and Values to compute self-attention, how the decoder runs in parallel with encoder outputs, and how softmaxed attention scores drive next-token prediction.

## ğŸ¯ Key Learnings

1. **Tensor Shape Management**  
   Splitting `d_model` into `h` attention heads and reshaping tensors (`view` + `transpose`) without mismatches is criticalâ€”one wrong dimension and you get a nasty â€œshape invalidâ€ error.

2. **Modular Code Design**  
   Breaking the model into clear components (Embeddings, PositionalEncoding, MultiHeadAttention, FeedForward, Residual + LayerNorm, Encoder/Decoder blocks) makes it far easier to debug, test, and extend.

3. **Iterative Debugging & Validation**  
   Inspecting tensor shapes at each stage, printing intermediate outputs, and writing small unit tests for each submodule saved countless hours chasing silent shape or type bugs.

## ğŸ“‚ Repository Structure

.
â”œâ”€â”€ README.md â† this file
â”œâ”€â”€ model.py â† Transformer implementation
â”œâ”€â”€ dataset.py â† data loading & masking logic
â”œâ”€â”€ train.py â† training loop, TensorBoard logging
â”œâ”€â”€ config.py â† hyperparameters & file paths
â”œâ”€â”€ requirements.txt â† Python dependencies
â””â”€â”€ runs/ â† TensorBoard logs


## âš™ï¸ Installation

1. **Clone the repo**  
   ```bash
   git clone https://github.com/yourusername/transformer-pytorch.git
   cd transformer-pytorch


ğŸ“ Architecture
InputEmbeddings + PositionalEncoding

NÃ— Encoder Blocks (Self-Attention â†’ Add&Norm â†’ FeedForward â†’ Add&Norm)

NÃ— Decoder Blocks (Masked Self-Attention â†’ Add&Norm â†’ Cross-Attention â†’ Add&Norm â†’ FeedForward â†’ Add&Norm)

ProjectionLayer with log-softmax over target vocabulary

Refer to the comments in model.py for detailed method-level explanations.
